{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_hub as hub\n",
    "#import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Model, Input\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers.merge import add\n",
    "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обработка входных данных\n",
    "1. Считывание файла перекодированного в utf-8 в базу данных\n",
    "1. Описание полученных данных\n",
    "1. Формирование Массивов для обучения модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Файл с входными данными \"data.txt\" был конвертирован в файл \"data_utf.txt\" с кодировкой utf-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt_to_dataframe(path):\n",
    "    #txt файл перекодирован в формат utf-8 для безпроблемной (почти безпроблемной читаемости)\n",
    "    columns=['Sentence #', 'text', 'tag']\n",
    "    data = pd.DataFrame([], columns=columns)\n",
    "    with io.open(path, encoding='utf-8') as file:\n",
    "        sentence = []\n",
    "        sentence_number = 0\n",
    "        for line in file:\n",
    "            if len(line.split(' '))!=2:\n",
    "                try:\n",
    "                    #data.append(sentence)\n",
    "                    df = pd.DataFrame(sentence, columns = columns)\n",
    "                    data = data.append(df, ignore_index = True)\n",
    "                    sentence_number +=1\n",
    "                    sentence = []\n",
    "                except:\n",
    "                    print('Error while trying to form DataFrame')\n",
    "            else:\n",
    "                word = line.split(' ')\n",
    "                try:\n",
    "                    sentence.append([sentence_number, word[0], word[1][:-1]])\n",
    "                except:\n",
    "                    print('Error while reading or parsing data from file')\n",
    "    return data         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вид DataFrame\n",
      "Wall time: 1min 50s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>text</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>﻿Man</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>i</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>hate</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>when</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>people</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>carry</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>ragedy</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>luggage</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>..</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>ima</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>just</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>rip</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>it</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>up</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>more</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>with</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>belt</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>loader</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>#itaintmines</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentence #          text tag\n",
       "0           0          ﻿Man   O\n",
       "1           0             i   O\n",
       "2           0          hate   O\n",
       "3           0          when   O\n",
       "4           0        people   O\n",
       "5           0         carry   O\n",
       "6           0        ragedy   O\n",
       "7           0       luggage   O\n",
       "8           0            ..   O\n",
       "9           0           ima   O\n",
       "10          0          just   O\n",
       "11          0           rip   O\n",
       "12          0            it   O\n",
       "13          0            up   O\n",
       "14          0          more   O\n",
       "15          0          with   O\n",
       "16          0           the   O\n",
       "17          0          belt   O\n",
       "18          0        loader   O\n",
       "19          0  #itaintmines   O"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data = read_txt_to_dataframe('data_utf.txt')\n",
    "print('Вид DataFrame')\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Этот класс позволяет считывать предложения (твиты) из базы данных по очереди:\n",
      "[('\\ufeffMan', 'O'), ('i', 'O'), ('hate', 'O'), ('when', 'O'), ('people', 'O'), ('carry', 'O'), ('ragedy', 'O'), ('luggage', 'O'), ('..', 'O'), ('ima', 'O'), ('just', 'O'), ('rip', 'O'), ('it', 'O'), ('up', 'O'), ('more', 'O'), ('with', 'O'), ('the', 'O'), ('belt', 'O'), ('loader', 'O'), ('#itaintmines', 'O')]\n"
     ]
    }
   ],
   "source": [
    "class SentenceGetter(object):\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 0\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(p, t) for w, p, t in zip(s[data.columns[0]].values.tolist(),\n",
    "                                                           s[data.columns[1]].values.tolist(),\n",
    "                                                           s[data.columns[2]].values.tolist())]\n",
    "        self.grouped = self.data.groupby(['Sentence #']).apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[self.n_sent]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "        \n",
    "getter = SentenceGetter(data)\n",
    "sent = getter.get_next()\n",
    "print('Этот класс позволяет считывать предложения (твиты) из базы данных по очереди:')\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание данных в базе: \n",
    "1. количество записей \n",
    "1. уникальные значения по каждой из колонок\n",
    "1. Частоту каждой из меток во всех токенах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Sentence #    text     tag\n",
      "count       124629  124629  124629\n",
      "unique        7243   28820      21\n",
      "top           4654       .       O\n",
      "freq            41    3495  115084\n",
      "Уникальных значений в колонках:\n",
      " Sentence #     7243\n",
      "text          28820\n",
      "tag              21\n",
      "Name: unique, dtype: object \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Описание данных в базе\n",
    "print(data.describe())\n",
    "print('Уникальных значений в колонках:\\n', data.describe().iloc[1], '\\n')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пропусков в базе данных быть не должно в каждой из колоник:\n",
      "\n",
      "Sentence #    0\n",
      "text          0\n",
      "tag           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Пропусков в базе данных быть не должно в каждой из колоник:\\n')\n",
    "print(data.count() - data.notna().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>O</td>\n",
       "      <td>115084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B-geo-loc</td>\n",
       "      <td>1274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>B-person</td>\n",
       "      <td>1102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>I-other</td>\n",
       "      <td>973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>B-other</td>\n",
       "      <td>941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B-company</td>\n",
       "      <td>831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>I-product</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I-person</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I-facility</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B-facility</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>B-product</td>\n",
       "      <td>380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>I-company</td>\n",
       "      <td>311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I-geo-loc</td>\n",
       "      <td>310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B-musicartist</td>\n",
       "      <td>287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>B-sportsteam</td>\n",
       "      <td>268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>I-musicartist</td>\n",
       "      <td>236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I-movie</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I-sportsteam</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B-movie</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I-tvshow</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>B-tvshow</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tag   count\n",
       "20              O  115084\n",
       "2       B-geo-loc    1274\n",
       "6        B-person    1102\n",
       "15        I-other     973\n",
       "5         B-other     941\n",
       "0       B-company     831\n",
       "17      I-product     701\n",
       "16       I-person     610\n",
       "11     I-facility     510\n",
       "1      B-facility     395\n",
       "7       B-product     380\n",
       "10      I-company     311\n",
       "12      I-geo-loc     310\n",
       "4   B-musicartist     287\n",
       "8    B-sportsteam     268\n",
       "14  I-musicartist     236\n",
       "13        I-movie     109\n",
       "18   I-sportsteam      84\n",
       "3         B-movie      83\n",
       "19       I-tvshow      71\n",
       "9        B-tvshow      69"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat = data.groupby(['tag']).tag.count().reset_index(name = 'count').sort_values(['count'], ascending = False)\n",
    "stat                   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы имеем базу твитов разбитых по токенам из 7243 твитов. Общее количество токенов 124629.\n",
    "Самое большое количество проставленных меток относится к классу \"O\" (очевидно).\n",
    "Далее идут метки ['B-geo-loc', 'B-person', 'I-other', 'B-other', 'B-company'] с частотой в диапазоне [800,2000]\n",
    "Скорей всего самые плохие результаты будут на самых немногочисленных метках с частотой меньше 200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее посчитаем распределение твитов по длине и выясним максиальную длину предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Получим список твитов разбитый по токенам\n",
    "sentences = getter.sentences\n",
    "\n",
    "# Распределение твитов по количеству токенов\n",
    "plt.hist([len(s) for s in sentences], bins=30, align='mid')\n",
    "plt.title('Tokens per sentence')\n",
    "plt.xlabel('Длина твита в токенах')\n",
    "plt.ylabel('Количество твитов')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Срредняя длина твита $\\approx 17$ токенов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое днинный твит состоит из 41 токенов\n"
     ]
    }
   ],
   "source": [
    "max_len = np.max([len(x) for x in sentences])\n",
    "print('Самое днинный твит состоит из %i токенов' % max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Словарь состоит из 28821 слов\n"
     ]
    }
   ],
   "source": [
    "# Составим лист из всех выделенных токенов\n",
    "words_list = list(set(data['text'].values))\n",
    "words_list.append('__PAD__')\n",
    "n_words = len(words_list)\n",
    "print('Словарь состоит из %i слов' % n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего имеем 21 метка\n",
      "0 :  B-other\n",
      "1 :  I-tvshow\n",
      "2 :  B-sportsteam\n",
      "3 :  I-geo-loc\n",
      "4 :  B-company\n",
      "5 :  I-movie\n",
      "6 :  I-product\n",
      "7 :  I-sportsteam\n",
      "8 :  B-facility\n",
      "9 :  I-other\n",
      "10 :  I-musicartist\n",
      "11 :  I-person\n",
      "12 :  B-tvshow\n",
      "13 :  B-product\n",
      "14 :  I-company\n",
      "15 :  B-geo-loc\n",
      "16 :  O\n",
      "17 :  B-musicartist\n",
      "18 :  B-movie\n",
      "19 :  B-person\n",
      "20 :  I-facility\n"
     ]
    }
   ],
   "source": [
    "# Составим лист всех меток классов\n",
    "tags_list = list(set(data['tag'].values))\n",
    "n_tags = len(tags_list)\n",
    "print('Всего имеем %i метка' % n_tags)\n",
    "for i, tag in enumerate(tags_list):\n",
    "    print(i, ': ', tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Зададим максимальную длину твита с небольшим запасом (в данном случае 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 50\n",
    "tag2idx = {t: i for i, t in enumerate(tags_list)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы передать твиты в модель необходимо дополнить каждый твит до длины MAX_LEN уникальным токеном \"__PAD__\". То же самое делаем с массивом меток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Just', 'In', ':', 'University', 'of', 'Illinois', 'Chancellor', 'Wise', 'resigning', 'Aug', '.', '12', 'http://t.co/1bwas7bJUl']\n",
      "Так выглядит токенищированный твит для обучения модели\n",
      "['Just' 'In' ':' 'University' 'of' 'Illinois' 'Chancellor' 'Wise'\n",
      " 'resigning' 'Aug' '.' '12' 'http://t.co/1bwas7bJUl' '__PAD__' '__PAD__'\n",
      " '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__'\n",
      " '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__'\n",
      " '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__'\n",
      " '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__'\n",
      " '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__' '__PAD__']\n"
     ]
    }
   ],
   "source": [
    "#Дополняем каждый твит до длины MAX_LEN\n",
    "X = np.array([[w[0] for w in s] for s in sentences])\n",
    "#print(X[10])\n",
    "new_X = []\n",
    "for seq in X:\n",
    "    new_seq = []\n",
    "    for i in range(max_len):\n",
    "        try:\n",
    "            new_seq.append(seq[i])\n",
    "        except:\n",
    "            new_seq.append(\"__PAD__\")\n",
    "    new_X.append(new_seq)\n",
    "X = np.array(new_X)\n",
    "print('Так выглядит токенизированный твит для обучения модели')\n",
    "print(X[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходная длина:  13 \n",
      "Вектор меток:  [16, 16, 16, 0, 9, 9, 19, 11, 16, 16, 16, 16, 16]\n",
      "Преобразованная длина:  50 \n",
      "Новый вектор меток:  [16 16 16  0  9  9 19 11 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
      " 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16 16\n",
      " 16 16]\n"
     ]
    }
   ],
   "source": [
    "#Дополняем каждый вектор меток до длины MAX_LEN\n",
    "y = [[tag2idx[w[1]] for w in s] for s in sentences]\n",
    "print('Исходная длина: ', len(y[10]), '\\nВектор меток: ', y[10])\n",
    "y = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"O\"])\n",
    "print('Преобразованная длина: ', len(y[10]), '\\nНовый вектор меток: ', y[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Разделяем выборку на \n",
    "#Обрезаем обучающую выборку для корректоного обучения слоя LSTM по параметру batch_size\n",
    "def get_train_test_data(X,y, batch_size):\n",
    "    \n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X, y, test_size=0.1, random_state=2018)\n",
    "    batch_size = 32\n",
    "    X_train = np.copy(X_tr[0:X_tr.shape[0]//batch_size*batch_size]) \n",
    "    X_val   = np.copy(X_te[0:X_te.shape[0]//batch_size*batch_size])\n",
    "    y_train = np.copy(y_tr[0:y_tr.shape[0]//batch_size*batch_size])\n",
    "    y_val   = np.copy(y_te[0:y_te.shape[0]//batch_size*batch_size])\n",
    "    #X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "    y_train = y_train.reshape(y_train.shape[0], y_train.shape[1],1)\n",
    "    #X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
    "    y_val = y_val.reshape(y_val.shape[0], y_val.shape[1],1)\n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель решения\n",
    "Bidirectional LSTM для семантического анализа текстово были предложены в большом колечетве статей\n",
    "Несколько примеров.\n",
    "https://doi.org/10.18653/v1/W17-4421\n",
    "https://doi.org/10.17863/CAM.7201\n",
    "\n",
    "Кроме того для увеличения качества модели использована предобученная сеть ELMO (https://allennlp.org/elmo) для создания вложенного слоя, который чувствителен к контексту в отличе от других слоев который были бы обучены на тестовой выборке.\n",
    "Конечная модель взята из материала: \n",
    "https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/\n",
    "\n",
    "Итоговый вид модели Состоит из\n",
    "1. Слоя ELMO Embedding\n",
    "1. Двух связанных слоев LSTM\n",
    "1. Полносвязного слоя для вывода итоговых классов\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ElmoEmbedding(x):\n",
    "    return elmo_model(inputs={\n",
    "                            \"tokens\": tf.squeeze(tf.cast(x, tf.string)),\n",
    "                            \"sequence_len\": tf.constant(batch_size*[max_len])\n",
    "                      },\n",
    "                      signature=\"tokens\",\n",
    "                      as_dict=True)[\"elmo\"]\n",
    "def initialise_sess():\n",
    "    sess = tf.Session()\n",
    "    K.set_session(sess)\n",
    "    #инициализация ELMO EMbedding\n",
    "    elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=True)\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    #sess.run(tf.tables_initializer())\n",
    "    sess.run(tf.group([tf.global_variables_initializer(), tf.tables_initializer()]))\n",
    "    return sess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0814 13:35:59.979997  8996 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 50, 1024)     0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_5 (Bidirectional) (None, 50, 512)      2623488     lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_6 (Bidirectional) (None, 50, 512)      1574912     bidirectional_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 50, 512)      0           bidirectional_5[0][0]            \n",
      "                                                                 bidirectional_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 50, 21)       10773       add_3[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 4,209,173\n",
      "Trainable params: 4,209,173\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Гиперпараметры модели\n",
    "lstm_size = 256 #Размер LSTM слоев\n",
    "epochs = 3 # Кол-во итераций обучения\n",
    "verbose = 1 # Вид вывода прогресса обучения\n",
    "batch_size = 32 # Частота обновления весов модели (точнее кол-во итерации между обновлениями)\n",
    "\n",
    "sess = tf.Session()\n",
    "K.set_session(sess)\n",
    "#инициализация ELMO EMbedding\n",
    "elmo_model = hub.Module(\"https://tfhub.dev/google/elmo/2\", trainable=False)\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "# Построение слоев\n",
    "input_text = Input(shape=(max_len,), dtype=tf.string)\n",
    "embedding = Lambda(ElmoEmbedding, output_shape=(max_len, 1024))(input_text)\n",
    "x = Bidirectional(LSTM(units=lstm_size, return_sequences=True,\n",
    "                       recurrent_dropout=0.2, dropout=0.2))(embedding)\n",
    "x_rnn = Bidirectional(LSTM(units=lstm_size, return_sequences=True,\n",
    "                           recurrent_dropout=0.2, dropout=0.2))(x)\n",
    "x = add([x, x_rnn])  # residual connection to the first biLSTM\n",
    "out = TimeDistributed(Dense(n_tags, activation=\"softmax\"))(x)\n",
    "#компиляция модели\n",
    "model = Model(input_text, out)\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"categorical_accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метод для Сохранения модели в файл\n",
    "def save_model(model_path):\n",
    "    v1 = tf.get_variable(\"v1\", shape=[3], initializer = tf.zeros_initializer)\n",
    "    v2 = tf.get_variable(\"v2\", shape=[5], initializer = tf.zeros_initializer)\n",
    "\n",
    "    inc_v1 = v1.assign(v1+1)\n",
    "    dec_v2 = v2.assign(v2-1)\n",
    "    \n",
    "    # Add an op to initialize the variables.\n",
    "    init_op = tf.global_variables_initializer()\n",
    "\n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        # Do some work with the model.\n",
    "        inc_v1.op.run()\n",
    "        dec_v2.op.run()\n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(sess, model_path)\n",
    "        print(\"Model saved in path: %s\" % save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Метод для Загрузки модели из файла\n",
    "def load_model(model_path):\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # Create some variables.\n",
    "    v1 = tf.get_variable(\"v1\", shape=[3])\n",
    "    v2 = tf.get_variable(\"v2\", shape=[5])\n",
    "    \n",
    "    # Add ops to save and restore all the variables.\n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Later, launch the model, use the saver to restore variables from disk, and\n",
    "    # do some work with the model.\n",
    "    with tf.Session() as sess:\n",
    "          # Restore variables from disk.\n",
    "          saver.restore(sess, model_path)\n",
    "          print(\"Model restored.\")\n",
    "          # Check the values of the variables\n",
    "          print(\"v1 : %s\" % v1.eval())\n",
    "          print(\"v2 : %s\" % v2.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6496, 50) (6496, 50, 1)\n",
      "(704, 50) (704, 50, 1)\n"
     ]
    }
   ],
   "source": [
    "#Финальное преобразование формы входных данных для обучения\n",
    "X_train, X_val, y_train, y_val = get_train_test_data(X,y, batch_size)\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6496 samples, validate on 704 samples\n",
      "Epoch 1/3\n",
      "6496/6496 [==============================] - 3303s 508ms/step - loss: 0.1192 - categorical_accuracy: 6.3424e-04 - val_loss: 0.0676 - val_categorical_accuracy: 5.3977e-04\n",
      "Epoch 2/3\n",
      "6496/6496 [==============================] - 2925s 450ms/step - loss: 0.0623 - categorical_accuracy: 0.0015 - val_loss: 0.0575 - val_categorical_accuracy: 0.0017\n",
      "Epoch 3/3\n",
      "6496/6496 [==============================] - 2858s 440ms/step - loss: 0.0505 - categorical_accuracy: 0.0017 - val_loss: 0.0572 - val_categorical_accuracy: 0.0022\n",
      "Model saved in path: model_elmo_lstm.ckpt\n"
     ]
    }
   ],
   "source": [
    "history =  model.fit(X_train, y_train, validation_data=(X_val, y_val), \n",
    "                     batch_size=batch_size, epochs=epochs, verbose=verbose)\n",
    "save_model('model_elmo_lstm.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае нам не интересно смотреть на оценку модели при обучении, т.к. по сути любая оценка будет давать качество предсказания метки \"O\". Здесь вообще не удалось сделать работающую метрику внутренними методами (это надо разбираться почему в похожих примерах работает, а тут нет). \n",
    "\n",
    "Качество мы будем оценивать по другим метрикам дальше."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гиперпараметры модели выбраны из соображений достаточной сложности и относительно быстрого выполнения на ноутбуке без видеокарты, поэтому выполнено только 3 итерации обучения. На GPU можно обучить данную модель быстрее, сделать больше итерации и сделать её мощнее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ИЛИ загрузим модель из сохраненного файла\n",
    "#load_model('model_elmo_lstm.ckpt')\n",
    "#load_model('/tmp/model.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Результаты обучения модели\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Переведём вектор с предсказаниями модели на тестовой выборке \n",
    "# в тип to np.array для дальнейшей обработки\n",
    "y_pred = model.predict(X_val)\n",
    "tags_pred = []\n",
    "tags_val = []\n",
    "for i,j in enumerate(y_val):\n",
    "    tags_pred.append([np.argmax(i) for i in y_pred[i]])\n",
    "    tags_val.append([i[0] for i in y_val[i]])\n",
    "#print(tags_val[0:2])\n",
    "pred = np.array(tags_pred)\n",
    "val = np.array(tags_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метриками качества в данной задаче будет подсчет значений PRESISION, RECALL, F-1 SCORE для каждой метки\n",
    "Для этого написана функция, которая будет считать данные метрики из векторов предсказаний модели PRED и размеченных ответов VAL (см. выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr_rc_f1(ind, pred, val):\n",
    "    binary_pred = np.array([[1 if i==ind else 0 for i in j] for j in pred])\n",
    "    binary_val  = np.array([[1 if i==ind else 0 for i in j] for j in val ])\n",
    "    tp = np.sum(np.logical_and(binary_pred == 1, binary_val == 1))\n",
    "    fp = np.sum(np.logical_and(binary_pred == 1, binary_val == 0))\n",
    "    fn = np.sum(np.logical_and(binary_pred == 0, binary_val == 1))\n",
    "    pr = 0 if (tp+fp)==0 else float(tp)/float(tp+fp)\n",
    "    rc = 0 if (tp+fn)==0 else float(tp)/float(tp+fn)\n",
    "    counts_pred = np.sum(binary_pred)\n",
    "    counts_val  = np.sum(binary_val)\n",
    "    return (pr, rc, 0 if pr+rc==0 else 2.*pr*rc/(pr+rc), counts_pred, counts_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_report(pred, val):\n",
    "    report = pd.DataFrame(columns=['tag', 'precision', 'recall', 'f1-score', 'pred counts', 'val counts'])\n",
    "    for i, tag in enumerate(tags_list):\n",
    "        pr, rc, f1, pr_counts, val_counts = pr_rc_f1(i, pred, val)\n",
    "        report.loc[i] = [tag, pr, rc, f1, pr_counts, val_counts]\n",
    "    print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также напишем две функции для удобного просмотра предсказаний по отдельным твитам (по конкретному номеру и случайному)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2018)\n",
    "def print_test_message(ind, y_pred, y_val):\n",
    "    test_num = ind\n",
    "    try:\n",
    "        #print('TEXT, PREDICTED TAG, ASSIGNED TAG')\n",
    "        print(\"{:15}: {:10} ({})\".format('Token', 'Pred', 'True'))\n",
    "        print('='*30)\n",
    "        for i,j in enumerate(y_pred[test_num]):\n",
    "            if X_val[test_num,i]=='__PAD__':\n",
    "                break\n",
    "            #print(X_val[test_num,i], ',', tags_list[np.argmax(j)], ',', tags_list[y_val[test_num, i][0]])\n",
    "            print(\"{:15}:{:10} ({})\".format(X_val[ind,i], \n",
    "                                           tags_list[np.argmax(j)],\n",
    "                                           tags_list[y_val[test_num, i][0]]))\n",
    "    except:\n",
    "        print('Error while getting message')\n",
    "def print_random_test_message(y_pred, y_val):\n",
    "    ri = random.randint(0,len(y_pred))\n",
    "    print('Твит №%i в тестовой выборке' % ri)\n",
    "    print_test_message(ri, y_pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем результаты предсказаний для несколькихз твитов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          : Pred       (True)\n",
      "==============================\n",
      "“@BAESIC80s    :O          (O)\n",
      ":              :O          (O)\n",
      "\"              :O          (O)\n",
      "@cocainee69    :O          (O)\n",
      ":              :O          (O)\n",
      "So             :O          (O)\n",
      "in             :O          (O)\n",
      "Detroit        :B-geo-loc  (B-geo-loc)\n",
      "we             :O          (O)\n",
      "shooting       :O          (O)\n",
      "the            :O          (O)\n",
      "new            :O          (O)\n",
      "batman         :O          (O)\n",
      "movie          :O          (O)\n",
      "..             :O          (O)\n",
      "Niggas         :B-person   (O)\n",
      "done           :O          (O)\n",
      "stole          :O          (O)\n",
      "the            :O          (O)\n",
      "BatMobile      :O          (B-other)\n",
      "!\"             :O          (O)\n",
      "LMFAOOOOO      :O          (O)\n",
      "WAIT”          :O          (O)\n",
      "MY             :O          (O)\n",
      "CITY           :O          (O)\n",
      "SAVAGE         :O          (O)\n"
     ]
    }
   ],
   "source": [
    "#Твит с конкретным номером в тестовой выборке\n",
    "print_test_message(524, y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Твит №653 в тестовой выборке\n",
      "Token          : Pred       (True)\n",
      "==============================\n",
      "ICAST          :B-other    (B-other)\n",
      "will           :O          (O)\n",
      "be             :O          (O)\n",
      "attending      :O          (O)\n",
      "the            :O          (O)\n",
      "Colorado       :B-other    (B-other)\n",
      "Energy         :I-other    (I-other)\n",
      "Expo           :I-other    (I-other)\n",
      "!              :O          (O)\n",
      "Join           :O          (O)\n",
      "us             :O          (O)\n",
      "on             :O          (O)\n",
      "May            :O          (O)\n",
      "13             :O          (O)\n",
      "to             :O          (O)\n",
      "#DiscoverOurEnergy:O          (O)\n",
      ".              :O          (O)\n",
      "Learn          :O          (O)\n",
      "more           :O          (O)\n",
      "and            :O          (O)\n",
      "register       :O          (O)\n",
      "at             :O          (O)\n",
      "http://t.co/6qT2Og3lwW:O          (O)\n"
     ]
    }
   ],
   "source": [
    "#Выхов случайного твита из тестовой выборки\n",
    "print_random_test_message(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              tag  precision    recall  f1-score pred counts val counts\n",
      "0         B-other   0.564103  0.448980  0.500000          78         98\n",
      "1        I-tvshow   0.000000  0.000000  0.000000           0          2\n",
      "2    B-sportsteam   0.583333  0.451613  0.509091          24         31\n",
      "3       I-geo-loc   0.642857  0.428571  0.514286          14         21\n",
      "4       B-company   0.772727  0.673267  0.719577          88        101\n",
      "5         I-movie   0.000000  0.000000  0.000000           0          6\n",
      "6       I-product   1.000000  0.190476  0.320000          16         84\n",
      "7    I-sportsteam   1.000000  0.285714  0.444444           2          7\n",
      "8      B-facility   0.666667  0.564103  0.611111          33         39\n",
      "9         I-other   0.647059  0.444444  0.526946          68         99\n",
      "10  I-musicartist   0.600000  0.136364  0.222222           5         22\n",
      "11       I-person   0.711864  0.763636  0.736842          59         55\n",
      "12       B-tvshow   1.000000  0.142857  0.250000           1          7\n",
      "13      B-product   0.631579  0.255319  0.363636          19         47\n",
      "14      I-company   0.700000  0.583333  0.636364          30         36\n",
      "15      B-geo-loc   0.752212  0.674603  0.711297         113        126\n",
      "16              O   0.990768  0.996583  0.993667       34444      34243\n",
      "17  B-musicartist   0.333333  0.238095  0.277778          15         21\n",
      "18        B-movie   0.000000  0.000000  0.000000           0          6\n",
      "19       B-person   0.582734  0.861702  0.695279         139         94\n",
      "20     I-facility   0.730769  0.690909  0.710280          52         55\n"
     ]
    }
   ],
   "source": [
    "print_report(pred, val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из этой сводки по метрикам можно сделать следующие выводы:\n",
    "1. Самые точыне прогнозы вышли по метке \"O\". Но это вполне ожидаемо.\n",
    "1. Самые многочисленные метки (\"B-geo-loc\",\"B-person\", \"B-other\", \"I-other\", \"B-company\") имеют самый большой F1-score в дипазоне [0.5, 0.72]. Т.е. на этих метках данная модель (не особо мощная) делает вполне правдоподобные предсказания, это можно увидеть на конкретных примерах из тестовой выборки. По ссылке (https://www.depends-on-the-definition.com/named-entity-recognition-with-residual-lstm-and-elmo/) подобная модель (с большим количеством переменных) была обучена на выборке в 7 раз больше с общей точностью 98% (но это с учетом всех меток).\n",
    "1. Совсем плохо дела обстоят с самыми непредставленными метками (\"B-movie\", \"I-movie\", \"B-sportsteam\", \"I-Sportsteam\", \"B-musical-artist\"). Этих меток слишком мало в обучающей и тестовых выборках. В теории можно сделать bootstrap для твитов с этими метками, чтобы всё было не так плохо.\n",
    "1. Вцелом модель достаточно точно выставляет последовательности \"B-...\" и \"I-...\" меток, что можно посмотреть на примерах, т.е. сочетание ELMO Embedding и LSTM достаточно удачное для данной задачи.\n",
    "1. Можно предпо ложить что увеличивая число итераций модели можно ещё повысить качество на такой маленькой выборке. Но для этого нужна машинка помощнее, а не ноутбук который грозит расплавиться.\n",
    "1. Точность модели можно (и нужно) увеличить путем увеличения объема обучающих выборок, более продолжительного обучения или увеличением размера LSTM слоев. Кроме того доступны более мощные ELMO_embedding слои на сайте (в данном приеме выбрана самая маленькая модель)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Токенизация строки и построение меток с помощью модели\n",
    "def custom_input(string):\n",
    "    tt = TweetTokenizer()\n",
    "    Y = tt.tokenize(string)\n",
    "    s_len = len(Y)\n",
    "    X = []\n",
    "    if s_len >= max_len:\n",
    "        print('Слишком много токенов, пришлось обрезать')\n",
    "        X.append(Y[0:max_len])\n",
    "        #Y = string.split(' ')[0:max_len]\n",
    "    else:\n",
    "        #Y = string.split(' ')[0:max_len]\n",
    "        for i in np.arange(max_len-s_len):\n",
    "            Y.append('__PAD__')\n",
    "        X.append(Y)    \n",
    "    for i in np.arange(batch_size-1):\n",
    "        X.append(('__PAD__ '*max_len).split(' ')[:-1])\n",
    "    pred = model.predict(np.array(X, dtype='str'))\n",
    "    markers = [tags_list[np.argmax(i)] for i in pred[0]]\n",
    "    for i,j in zip(X[0][:s_len],markers[:s_len]):\n",
    "        print('{:15} {:10}'.format(i,j))\n",
    "    return markers[:s_len], X[0][:s_len]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее можно построить предсказания меток для произвольной строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dear            O         \n",
      "Martin          B-person  \n",
      "Alekseevich     I-person  \n",
      ",               O         \n",
      "@Trump          O         \n",
      "Tower           O         \n",
      "write           O         \n",
      "you             O         \n",
      "from            O         \n",
      "Apple           B-company \n",
      "Co              I-company \n",
      ".               O         \n",
      "to              O         \n",
      "invite          O         \n",
      "you             O         \n",
      "to              O         \n",
      "our             O         \n",
      "micheladas      O         \n",
      "party           O         \n",
      "on              O         \n",
      "New             B-other   \n",
      "Years           I-other   \n",
      "Eve             I-other   \n",
      ".               O         \n"
     ]
    }
   ],
   "source": [
    "#вот так\n",
    "string = 'Dear Martin Alekseevich, @Trump Tower write you from Apple Co. to invite you to our micheladas party on New Years Eve.'\n",
    "m,s = custom_input(string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
